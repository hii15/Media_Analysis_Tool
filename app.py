import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
from scipy.stats import beta

# --- [0. ê¸°ë³¸ ì„¤ì •] ---
st.set_page_config(page_title="Ad Intelligence Pro v34.1", layout="wide")

# --- [1. ë°ì´í„° ì—”ì§„: ë¡œë”© ë° ì „ì²˜ë¦¬] ---
def load_and_clean_data(uploaded_file):
    try:
        if uploaded_file.name.endswith('.xlsx'):
            all_sheets = pd.read_excel(uploaded_file, sheet_name=None)
            df = pd.concat(all_sheets.values(), ignore_index=True)
        else:
            df = pd.read_csv(uploaded_file)
        
        df.columns = [c.strip() for c in df.columns]
        mapping = {'ë‚ ì§œ':['ë‚ ì§œ','ì¼ì'], 'ìƒí’ˆ':['ìƒí’ˆëª…','ìƒí’ˆ'], 'ì†Œì¬':['ì†Œì¬ëª…','ì†Œì¬'],
                   'ë…¸ì¶œ':['ë…¸ì¶œìˆ˜','ë…¸ì¶œ'], 'í´ë¦­':['í´ë¦­ìˆ˜','í´ë¦­'], 'ì¡°íšŒ':['ì¡°íšŒìˆ˜','ì¡°íšŒ'], 'ë¹„ìš©':['ë¹„ìš©','ì§€ì¶œ']}
        
        final_df = pd.DataFrame()
        for k, v in mapping.items():
            for col in v:
                if col in df.columns: final_df[k] = df[col]; break
        
        final_df['ë‚ ì§œ'] = pd.to_datetime(final_df['ë‚ ì§œ'], errors='coerce')
        for col in ['ë…¸ì¶œ', 'í´ë¦­', 'ì¡°íšŒ', 'ë¹„ìš©']:
            final_df[col] = pd.to_numeric(final_df[col].astype(str).str.replace(r'[^\d.]', '', regex=True), errors='coerce').fillna(0)
        
        final_df['CTR(%)'] = (final_df['í´ë¦­'] / (final_df['ë…¸ì¶œ'] + 1e-9) * 100)
        final_df['ID'] = "[" + final_df['ìƒí’ˆ'].astype(str).str.upper() + "] " + final_df['ì†Œì¬'].astype(str)
        return final_df.dropna(subset=['ë‚ ì§œ']).sort_values(['ID', 'ë‚ ì§œ'])
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì—ëŸ¬: {e}"); return pd.DataFrame()

# --- [2. í•µì‹¬ ì—”ì§„: Empirical Bayes (Moment Matching Kappa)] ---
def analyze_empirical_bayes(df):
    global_ctr = df['í´ë¦­'].sum() / (df['ë…¸ì¶œ'].sum() + 1e-9)
    # IDë³„ CTR ë¶„ì‚° ê³„ì‚° (Prior Strength ì¶”ì •ìš©)
    id_stats = df.groupby('ID').agg({'í´ë¦­':'sum', 'ë…¸ì¶œ':'sum', 'ë¹„ìš©':'last'})
    id_ctrs = id_stats['í´ë¦­'] / (id_stats['ë…¸ì¶œ'] + 1e-9)
    var_ctr = max(id_ctrs.var(), 1e-7)
    
    # Moment Matching: kappa = [p(1-p)/var] - 1
    kappa = (global_ctr * (1 - global_ctr) / var_ctr) - 1
    kappa = np.clip(kappa, 10, 1000) # ìˆ˜ì¹˜ì  ì•ˆì •ì„± ê°€ì´ë“œ
    
    alpha_0, beta_0 = global_ctr * kappa, (1 - global_ctr) * kappa
    
    agg = id_stats.reset_index()
    agg['post_alpha'] = alpha_0 + agg['í´ë¦­']
    agg['post_beta'] = beta_0 + (agg['ë…¸ì¶œ'] - agg['í´ë¦­'])
    agg['exp_ctr'] = agg['post_alpha'] / (agg['post_alpha'] + agg['post_beta'])
    
    # Thompson Sampling
    samples = np.random.beta(agg['post_alpha'].values[:, None], 
                             agg['post_beta'].values[:, None], size=(len(agg), 5000))
    agg['prob_is_best'] = np.bincount(np.argmax(samples, axis=0), minlength=len(agg)) / 5000
    
    # ìµœê·¼ ë¹„ìš©(ìµœê·¼ 3ì¼ í‰ê· ) ê°€ì ¸ì˜¤ê¸°
    max_date = df['ë‚ ì§œ'].max()
    last_costs = df[df['ë‚ ì§œ'] >= max_date - timedelta(days=3)].groupby('ID')['ë¹„ìš©'].mean()
    agg = agg.merge(last_costs.rename('last_3d_avg_cost'), on='ID', how='left').fillna(0)
    return agg, (alpha_0, beta_0, kappa)

# --- [3. íƒì§€ ì—”ì§„: Binomial CUSUM & Bootstrap ARL] ---
def get_binomial_cusum(clicks, imps, p0, p1_ratio=0.85):
    """ Binomial Log-Likelihood Ratio CUSUM (í•˜ë½ ê°ì§€ ì „ìš©) """
    p1 = p0 * p1_ratio
    p0 = np.clip(p0, 1e-6, 1-1e-6)
    p1 = np.clip(p1, 1e-6, 1-1e-6)
    
    # ìš°ë„ë¹„ ê³„ì‚°
    llr = clicks * np.log(p1/p0) + (imps - clicks) * np.log((1-p1)/(1-p0))
    s = 0
    cusum = []
    for val in llr:
        s = min(0, s + val) # One-sided (í•˜ë½ë§Œ ëˆ„ì )
        cusum.append(s)
    return np.array(cusum)

@st.cache_data
def estimate_h_arl(p0, imps_series, target_arl=30, sims=500):
    """ Monte Carlo ê¸°ë°˜ ARL ì„ê³„ì¹˜ ì¶”ì • """
    p1 = p0 * 0.85
    p0 = np.clip(p0, 1e-6, 1-1e-6)
    p1 = np.clip(p1, 1e-6, 1-1e-6)
    llr_s, llr_f = np.log(p1/p0), np.log((1-p1)/(1-p0))
    
    for h in np.arange(2.0, 15.0, 1.0):
        rls = []
        for _ in range(sims):
            s, t = 0, 0
            while t < 100: # Capped ARL (ìµœëŒ€ 100ì¼)
                t += 1
                n = np.random.choice(imps_series) # ë…¸ì¶œìˆ˜ ë³€ë™ì„± ë°˜ì˜
                c = np.random.binomial(int(n), p0)
                s = min(0, s + (c * llr_s + (int(n) - c) * llr_f))
                if s < -h: break
            rls.append(t)
        if np.mean(rls) >= target_arl: return h
    return 5.0

# --- [4. ë©”ì¸ UI íë¦„] ---
uploaded_file = st.file_uploader("ìº í˜ì¸ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (CSV/XLSX)", type=['csv', 'xlsx'])

if uploaded_file:
    df = load_and_clean_data(uploaded_file)
    if not df.empty:
        # ì—”ì§„ ì‹¤í–‰
        res_agg, (a0, b0, kappa_est) = analyze_empirical_bayes(df)
        ids = sorted(df['ID'].unique())
        
        tabs = st.tabs(["ğŸ“Š ì„±ê³¼ ëŒ€ì‹œë³´ë“œ", "ğŸ§¬ EB Shrinkage ì§„ë‹¨", "ğŸ“‰ í•˜ë½ ê°ì§€(CUSUM)", "ğŸ¯ ì˜ˆì‚° ì •ì±… ì œì•ˆ", "ğŸ§ª ì‹œìŠ¤í…œ ë¦¬í¬íŠ¸"])

        with tabs[0]: # ëŒ€ì‹œë³´ë“œ
            st.info("**[ê°€ì´ë“œ]** ìƒí’ˆë³„ ë¬¼ëŸ‰ ë¹„ì¤‘ê³¼ ê¸°ëŒ€ CTRì„ ë¹„êµí•©ë‹ˆë‹¤.")
            c1, c2 = st.columns(2)
            pie_m = c1.selectbox("ë¹„ì¤‘ ì§€í‘œ", ["ë¹„ìš©", "ë…¸ì¶œ", "í´ë¦­"])
            c1.plotly_chart(px.pie(df.groupby('ID')[pie_m].sum().reset_index(), values=pie_m, names='ID', hole=0.4), use_container_width=True)
            c2.plotly_chart(px.bar(res_agg, x='ID', y='exp_ctr', title="Empirical Bayes ì¶”ì • ê¸°ëŒ€ CTR(%)"), use_container_width=True)

        with tabs[1]: # EB Shrinkage
            st.info(f"**Prior Strength (Îº) ìë™ ì¶”ì •ì¹˜: {kappa_est:.2f}**")
            st.write("ë°ì´í„°ì˜ ë¶„ì‚°ì„ ê³ ë ¤í•˜ì—¬ ê°œë³„ ì†Œì¬ì˜ CTRì„ ë³´ì •í•©ë‹ˆë‹¤. (Shrinkage íš¨ê³¼)")
            
            fig_post = go.Figure()
            for _, row in res_agg.iterrows():
                samples = np.random.beta(row['post_alpha'], row['post_beta'], 3000)
                fig_post.add_trace(go.Box(x=samples, name=row['ID'], boxpoints=False))
            fig_post.update_layout(title="IDë³„ ì‚¬í›„ ë¶„í¬ (Posteriors)", xaxis_title="Expected CTR")
            st.plotly_chart(fig_post, use_container_width=True)

        with tabs[2]: # CUSUM (ì—ëŸ¬ ë°œìƒí–ˆë˜ ì§€ì )
            st.info("**[ê°€ì´ë“œ]** í•˜ë½ ì „ìš© ìš°ë„ë¹„ ê°ì§€ê¸° (One-sided Fatigue Detector)")
            target_id = st.selectbox("ë¶„ì„ ëŒ€ìƒ ì„ íƒ", ids)
            t_df = df[df['ID']==target_id].sort_values('ë‚ ì§œ')
            
            # í•¨ìˆ˜ í˜¸ì¶œ ì „ p0 ì •ì˜
            p0_val = res_agg[res_agg['ID']==target_id]['exp_ctr'].values[0]
            
            # 1. h ì‚°ì¶œ (Bootstrap ARL)
            h_opt = estimate_h_arl(p0_val, t_df['ë…¸ì¶œ'].values)
            
            # 2. CUSUM ëˆ„ì  (í•¨ìˆ˜ í˜¸ì¶œ)
            cusum_v = get_binomial_cusum(t_df['í´ë¦­'].values, t_df['ë…¸ì¶œ'].values, p0_val)
            
            is_alarm = cusum_v[-1] < -h_opt
            
            fig_c = go.Figure()
            fig_c.add_trace(go.Scatter(x=t_df['ë‚ ì§œ'], y=cusum_v, name="Log-Likelihood Ratio Sum", fill='tozeroy'))
            fig_c.add_hline(y=-h_opt, line_dash="dash", line_color="red", annotation_text=f"ARL-30 Threshold (h={h_opt})")
            st.plotly_chart(fig_c, use_container_width=True)
            if is_alarm: st.error("ğŸš¨ **êµ¬ì¡°ì  í•˜ë½ ê°ì§€**: ì„±ê³¼ê°€ í†µê³„ì  ì‹ ë¢° í•œê³„ë¥¼ ë²—ì–´ë‚˜ í•˜ë½ ì¤‘ì…ë‹ˆë‹¤.")

        with tabs[3]: # ì˜ˆì‚° ìµœì í™”
            st.info("**[ê°€ì´ë“œ]** ê¸°ëŒ€ ì„±ê³¼ ë° ìŠ¹ë¦¬ í™•ë¥  ê¸°ë°˜ ìì› ë°°ë¶„ ì •ì±…")
            if st.button("ì˜ˆì‚° ì •ì±… ì‹¤í–‰"):
                res_agg['score'] = res_agg['exp_ctr'] * res_agg['prob_is_best']
                avg_score = res_agg['score'].mean() + 1e-9
                res_agg['proposed'] = res_agg['ë¹„ìš©'] * (res_agg['score'] / avg_score)
                
                # Safety Rail (Budget Inertia)
                res_agg['final_proposed'] = res_agg.apply(lambda r: np.clip(r['proposed'], r['ë¹„ìš©']*0.7, r['ë¹„ìš©']*1.3), axis=1)
                
                st.table(res_agg[['ID', 'exp_ctr', 'prob_is_best', 'final_proposed']].style.format(
                    {'exp_ctr':'{:.4f}', 'prob_is_best':'{:.2f}', 'final_proposed':'{:,.0f}'}))

        with tabs[4]: # ì‹œìŠ¤í…œ ë¦¬í¬íŠ¸
            st.subheader("ğŸ“Š Methodological Transparency")
            st.markdown(f"""
            - **Estimation**: Empirical Bayes (Moment Matching). $\kappa$ = {kappa_est:.2f}
            - **Detection**: Binomial Log-Likelihood Ratio CUSUM.
            - **Thresholding**: Monte Carlo-estimated Capped ARL.
            - **Exposure Variance**: Bootstrap sampling applied.
            """)
            st.success("ì´ ì‹œìŠ¤í…œì€ í†µê³„ì  ê³µì • ê´€ë¦¬(SPC) ì›ì¹™ì„ ì¤€ìˆ˜í•©ë‹ˆë‹¤.")