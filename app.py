import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
from scipy.stats import beta
from statsmodels.tsa.seasonal import seasonal_decompose

# --- [0. ì‹œìŠ¤í…œ ê¸°ë³¸ ì„¤ì •] ---
st.set_page_config(page_title="Ad Intelligence System v35.1", layout="wide")

st.title("ðŸ›¡ï¸ ë§¤ì²´ ë¼ì´ë¸Œ ê´€ë ¨ ì˜ì‚¬ê²°ì • ë³´ì¡° ë„êµ¬")
st.markdown("---")

# --- [1. ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ë ˆì´ì–´] ---
def load_and_clean_data(uploaded_file):
    try:
        if uploaded_file.name.endswith('.xlsx'):
            all_sheets = pd.read_excel(uploaded_file, sheet_name=None)
            df = pd.concat(all_sheets.values(), ignore_index=True)
        else:
            df = pd.read_csv(uploaded_file)
        
        df.columns = [c.strip() for c in df.columns]
        mapping = {
            'ë‚ ì§œ': ['ë‚ ì§œ', 'ì¼ìž'], 'ìƒí’ˆ': ['ìƒí’ˆëª…', 'ìƒí’ˆ'], 'ì†Œìž¬': ['ì†Œìž¬ëª…', 'ì†Œìž¬'],
            'ë…¸ì¶œ': ['ë…¸ì¶œìˆ˜', 'ë…¸ì¶œ'], 'í´ë¦­': ['í´ë¦­ìˆ˜', 'í´ë¦­'], 'ì¡°íšŒ': ['ì¡°íšŒìˆ˜', 'ì¡°íšŒ'], 'ë¹„ìš©': ['ë¹„ìš©', 'ì§€ì¶œ']
        }
        
        final_df = pd.DataFrame()
        for k, v in mapping.items():
            for col in v:
                if col in df.columns: final_df[k] = df[col]; break
        
        final_df['ë‚ ì§œ'] = pd.to_datetime(final_df['ë‚ ì§œ'], errors='coerce')
        for col in ['ë…¸ì¶œ', 'í´ë¦­', 'ì¡°íšŒ', 'ë¹„ìš©']:
            final_df[col] = pd.to_numeric(final_df[col].astype(str).str.replace(r'[^\d.]', '', regex=True), errors='coerce').fillna(0)
        
        final_df['CTR(%)'] = (final_df['í´ë¦­'] / (final_df['ë…¸ì¶œ'] + 1e-9) * 100)
        final_df['ID'] = "[" + final_df['ìƒí’ˆ'].astype(str).str.upper() + "] " + final_df['ì†Œìž¬'].astype(str)
        return final_df.dropna(subset=['ë‚ ì§œ']).sort_values(['ID', 'ë‚ ì§œ'])
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}"); return pd.DataFrame()

# --- [2. í†µê³„ ì—”ì§„ í•¨ìˆ˜ ì •ì˜] ---
def analyze_empirical_bayes(df):
    global_ctr = df['í´ë¦­'].sum() / (df['ë…¸ì¶œ'].sum() + 1e-9)
    id_stats = df.groupby('ID').agg({'í´ë¦­': 'sum', 'ë…¸ì¶œ': 'sum', 'ë¹„ìš©': 'last'})
    id_ctrs = id_stats['í´ë¦­'] / (id_stats['ë…¸ì¶œ'] + 1e-9)
    var_ctr = max(id_ctrs.var(), 1e-7)
    kappa = (global_ctr * (1 - global_ctr) / var_ctr) - 1
    kappa = np.clip(kappa, 10, 1000)
    alpha_0, beta_0 = global_ctr * kappa, (1 - global_ctr) * kappa
    
    agg = id_stats.reset_index()
    agg['post_alpha'] = alpha_0 + agg['í´ë¦­']
    agg['post_beta'] = beta_0 + (agg['ë…¸ì¶œ'] - agg['í´ë¦­'])
    agg['exp_ctr'] = agg['post_alpha'] / (agg['post_alpha'] + agg['post_beta'])
    
    samples = np.random.beta(agg['post_alpha'].values[:, None], 
                             agg['post_beta'].values[:, None], size=(len(agg), 5000))
    agg['prob_is_best'] = np.bincount(np.argmax(samples, axis=0), minlength=len(agg)) / 5000
    
    max_date = df['ë‚ ì§œ'].max()
    last_costs = df[df['ë‚ ì§œ'] >= max_date - timedelta(days=7)].groupby('ID')['ë¹„ìš©'].mean()
    agg = agg.merge(last_costs.rename('avg_cost_7d'), on='ID', how='left').fillna(0)
    return agg, (alpha_0, beta_0, kappa)

def get_binomial_cusum(clicks, imps, p0, p1_ratio=0.85):
    p1 = np.clip(p0 * p1_ratio, 1e-6, 1-1e-6)
    p0 = np.clip(p0, 1e-6, 1-1e-6)
    llr = clicks * np.log(p1/p0) + (imps - clicks) * np.log((1-p1)/(1-p0))
    s = 0
    cusum = []
    for val in llr:
        s = min(0, s + val)
        cusum.append(s)
    return np.array(cusum)

@st.cache_data
def estimate_h_arl(p0, imps_series, target_arl=30, sims=500):
    p1 = np.clip(p0 * 0.85, 1e-6, 1-1e-6)
    p0 = np.clip(p0, 1e-6, 1-1e-6)
    llr_s, llr_f = np.log(p1/p0), np.log((1-p1)/(1-p0))
    for h in np.arange(2.0, 15.0, 1.0):
        rls = []
        for _ in range(sims):
            s, t = 0, 0
            while t < 100:
                t += 1
                n = np.random.choice(imps_series)
                c = np.random.binomial(int(n), p0)
                s = min(0, s + (c * llr_s + (int(n) - c) * llr_f))
                if s < -h: break
            rls.append(t)
        if np.mean(rls) >= target_arl: return h
    return 5.0

def get_time_decomposition(df, target_col='CTR(%)'):
    if len(df) < 14: return None
    df_ts = df.set_index('ë‚ ì§œ')[target_col].resample('D').mean().interpolate()
    try:
        return seasonal_decompose(df_ts, model='additive', period=7)
    except: return None

# --- [3. ë©”ì¸ UI ë° íƒ­ë³„ ë¶„ì„ ë¡œì§] ---
uploaded_file = st.file_uploader("ìº íŽ˜ì¸ ì„±ê³¼ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['csv', 'xlsx'])

if uploaded_file:
    df = load_and_clean_data(uploaded_file)
    if not df.empty:
        res_agg, (a0, b0, k_est) = analyze_empirical_bayes(df)
        ids = sorted(df['ID'].unique())
        
        tabs = st.tabs(["ðŸ“Š í†µí•© ëŒ€ì‹œë³´ë“œ", "ðŸ§¬ í†µê³„ì  ì‹ ë¢°ë„ ë¶„ì„", "ðŸ“‰ ì¶”ì„¸ ë° í•˜ë½ ê°ì§€", "ðŸŽ¯ ì˜ˆì‚° íš¨ìœ¨ ê³¡ì„ "])

        with tabs[0]:
            st.markdown("### ðŸ“Š í†µí•© ëŒ€ì‹œë³´ë“œ")
            st.caption("ì „ì²´ ìº íŽ˜ì¸ì˜ í˜„í™©ì„ í•œëˆˆì— íŒŒì•…í•©ë‹ˆë‹¤. ìš°ì¸¡ ì°¨íŠ¸ì˜ CTRì€ í†µê³„ì ìœ¼ë¡œ ë³´ì •ë˜ì–´ ì‹ ë¢°ë„ê°€ ë†’ìŠµë‹ˆë‹¤.")
            col1, col2 = st.columns(2)
            metric = col1.selectbox("ë¹„ì¤‘ ë¶„ì„ ì§€í‘œ", ["ë¹„ìš©", "ë…¸ì¶œ", "í´ë¦­"])
            col1.plotly_chart(px.pie(df.groupby('ID')[metric].sum().reset_index(), values=metric, names='ID', hole=0.4), use_container_width=True)
            col2.plotly_chart(px.bar(res_agg, x='ID', y='exp_ctr', title="í†µê³„ ë³´ì •ëœ ê¸°ëŒ€ CTR (%)"), use_container_width=True)

        with tabs[1]:
            st.markdown("### ðŸ§¬ ë¶„ì„ ë°©ë²•ë¡ : Empirical Bayes (ìˆ˜ì¹˜ ë³´ì • ì•Œê³ ë¦¬ì¦˜)")
            st.write("""
            **ì™œ ì´ ë¶„ì„ì´ í•„ìš”í•œê°€ìš”?** ë…¸ì¶œìˆ˜ê°€ ì ì€ ì†Œìž¬ëŠ” ë‹¨ ëª‡ ë²ˆì˜ í´ë¦­ë§Œìœ¼ë¡œë„ CTRì´ 0%ê°€ ë˜ê±°ë‚˜ 50%ê°€ ë˜ëŠ” ë“± ìˆ˜ì¹˜ê°€ ë§¤ìš° ë¶ˆì•ˆì •í•©ë‹ˆë‹¤. ì´ë¥¼ 'ì†Œí‘œë³¸ ì™œê³¡'ì´ë¼ê³  í•©ë‹ˆë‹¤.
            
            **ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?** **Empirical Bayes** ê¸°ë²•ì€ ë°ì´í„° ì „ì²´ì˜ í‰ê· ì„ 'ì‚¬ì „ ì •ë³´'ë¡œ í™œìš©í•©ë‹ˆë‹¤. ë…¸ì¶œì´ ì ì€ ì†Œìž¬ëŠ” ì „ì²´ í‰ê·  ìª½ìœ¼ë¡œ ìˆ˜ì¹˜ë¥¼ ë³´ì •(Shrinkage)í•˜ê³ , ë…¸ì¶œì´ ì¶©ë¶„ížˆ ìŒ“ì¸ ì†Œìž¬ëŠ” ì‹¤ì œ ìˆ˜ì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ì˜í•©ë‹ˆë‹¤. 
            ì´ë¥¼ í†µí•´ **"ìš´ ì¢‹ê²Œ ë†’ê²Œ ë‚˜ì˜¨ ìˆ˜ì¹˜"ì™€ "ì§„ì§œ ì‹¤ë ¥"ì„ êµ¬ë¶„**í•´ë‚¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
            """)
            
            st.divider()
            st.info(f"ì „ì²´ ë°ì´í„° ê¸°ë°˜ ì¶”ì •ëœ ì‚¬ì „ ì‹ ë¢°ë„(Îº): {k_est:.2f}")
            fig_post = go.Figure()
            for _, row in res_agg.iterrows():
                samples = np.random.beta(row['post_alpha'], row['post_beta'], 3000)
                fig_post.add_trace(go.Box(x=samples, name=row['ID'], boxpoints=False))
            fig_post.update_layout(title="ì†Œìž¬ë³„ ì„±ê³¼ ì‹ ë¢° êµ¬ê°„ (ë°•ìŠ¤ê°€ ì¢ì„ìˆ˜ë¡ ìˆ˜ì¹˜ê°€ í™•ì‹¤í•¨ì„ ì˜ë¯¸)", xaxis_title="ê¸°ëŒ€ CTR ë²”ìœ„")
            st.plotly_chart(fig_post, use_container_width=True)

        with tabs[2]:
            st.markdown("### ðŸ“‰ ë¶„ì„ ë°©ë²•ë¡ : ì‹œê³„ì—´ ë¶„í•´ ë° CUSUM í•˜ë½ ê°ì§€")
            st.write("""
            **1. ì‹œê³„ì—´ ë¶„í•´ (Trend Extraction)** ê´‘ê³  ì„±ê³¼ëŠ” ìš”ì¼(ì£¼ë§/í‰ì¼)ì— ë”°ë¼ ì¶¤ì„ ì¶¥ë‹ˆë‹¤. ë‹¨ìˆœížˆ ì–´ì œë³´ë‹¤ CTRì´ ë–¨ì–´ì¡Œë‹¤ê³  í•´ì„œ ì„±ê³¼ í•˜ë½ìœ¼ë¡œ íŒë‹¨í•˜ë©´ ì˜¤ë¥˜ê°€ ìƒê¹ë‹ˆë‹¤.  
            ë³¸ ì‹œìŠ¤í…œì€ **ê°€ë²•ì  ì‹œê³„ì—´ ë¶„í•´**ë¥¼ í†µí•´ ìš”ì¼ ë°˜ë³µì„±ì„ ì œê±°í•˜ê³ , ì†Œìž¬ê°€ ê°€ì§„ **ìˆœìˆ˜ ì„±ê³¼ ì¶”ì„¸(Trend)**ë§Œ ì¶”ì¶œí•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.
            
            **2. CUSUM í•˜ë½ ê°ì§€ (Structural Drift Detection)** ì†Œìž¬ í”¼ë¡œë„ëŠ” ì„œì„œížˆ ì¼ì–´ë‚©ë‹ˆë‹¤. **CUSUM(ëˆ„ì í•©)** ë°©ì‹ì€ ë§¤ì¼ ë°œìƒí•˜ëŠ” ë¯¸ì„¸í•œ í•˜ë½ ì‹ í˜¸ë¥¼ ëˆ„ì ìœ¼ë¡œ í•©ì‚°í•˜ì—¬, í†µê³„ì  ìž„ê³„ì¹˜ë¥¼ ë„˜ì–´ì„œëŠ” ìˆœê°„ ì•ŒëžŒì„ ìš¸ë¦½ë‹ˆë‹¤.  
            ë‹¨ìˆœí•œ ë³€ë™(Noise)ì¸ì§€, êµ¬ì¡°ì ì¸ ì„±ê³¼ í•˜ë½(Signal)ì¸ì§€ë¥¼ ê³¼í•™ì ìœ¼ë¡œ íŒë³„í•©ë‹ˆë‹¤.
            """)
            
            st.divider()
            t_id = st.selectbox("ë¶„ì„ ëŒ€ìƒ ì†Œìž¬ ì„ íƒ", ids)
            sub = df[df['ID'] == t_id].sort_values('ë‚ ì§œ')
            p0_val = res_agg[res_agg['ID'] == t_id]['exp_ctr'].values[0]
            
            decomp = get_time_decomposition(sub)
            if decomp:
                fig_trend = go.Figure()
                fig_trend.add_trace(go.Scatter(x=decomp.trend.index, y=decomp.trend, name="ìš”ì¼ íš¨ê³¼ê°€ ì œê±°ëœ ìˆœìˆ˜ ì¶”ì„¸", line=dict(width=4)))
                fig_trend.add_trace(go.Scatter(x=sub['ë‚ ì§œ'], y=sub['CTR(%)'], name="ì›ë³¸ CTR ë°ì´í„°", opacity=0.2))
                fig_trend.update_layout(title="ì†Œìž¬ ì„±ê³¼ ì¶”ì„¸ ë¶„ì„")
                st.plotly_chart(fig_trend, use_container_width=True)
            
            h_opt = estimate_h_arl(p0_val, sub['ë…¸ì¶œ'].values)
            cusum_v = get_binomial_cusum(sub['í´ë¦­'].values, sub['ë…¸ì¶œ'].values, p0_val)
            fig_cusum = go.Figure()
            fig_cusum.add_trace(go.Scatter(x=sub['ë‚ ì§œ'], y=cusum_v, name="í•˜ë½ ì‹ í˜¸ ëˆ„ì ì¹˜", fill='tozeroy', line_color='red'))
            fig_cusum.add_hline(y=-h_opt, line_dash="dash", line_color="black", annotation_text="í†µê³„ì  ìœ„í—˜ ê²½ê³„ì„ ")
            fig_cusum.update_layout(title="ì†Œìž¬ í”¼ë¡œë„ ë° í•˜ë½ ì‹ í˜¸ íƒì§€ (ê·¸ëž˜í”„ê°€ ê²½ê³„ì„  ë°‘ìœ¼ë¡œ ë‚´ë ¤ê°€ë©´ êµì²´ ê¶Œìž¥)")
            st.plotly_chart(fig_cusum, use_container_width=True)

        with tabs[3]:
            st.markdown("### ðŸŽ¯ ë¶„ì„ ë°©ë²•ë¡ : ì˜ˆì‚° íš¨ìœ¨ ê³¡ì„  ë° ìµœì í™”")
            st.write("""
            **ë¹„ìš© íƒ„ë ¥ì„± ë¶„ì„ (Spend Elasticity)** ëˆì„ ë§Žì´ ì“´ë‹¤ê³  í•´ì„œ í´ë¦­ë¥ ì´ ê³„ì† ìœ ì§€ë˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. íŠ¹ì • ê¸ˆì•¡ ì´ìƒì—ì„œëŠ” íš¨ìœ¨ì´ ê¸‰ê²©ížˆ ë–¨ì–´ì§€ëŠ” êµ¬ê°„ì´ ì¡´ìž¬í•©ë‹ˆë‹¤.  
            ë³¸ íƒ­ì—ì„œëŠ” **ì§‘í–‰ ê·œëª¨ ëŒ€ë¹„ ê¸°ëŒ€ CTRì˜ ë¶„í¬**ë¥¼ ì‹œê°í™”í•˜ì—¬, í˜„ìž¬ ì˜ˆì‚°ì´ íš¨ìœ¨ì ìœ¼ë¡œ ë°°ë¶„ë˜ê³  ìžˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
            
            **Thompson Sampling ê¸°ë°˜ ì •ì±… ì œì•ˆ** ë‹¨ìˆœížˆ CTRì´ ë†’ì€ ê³³ì— ëˆì„ ëª°ì•„ì£¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **"ì´ ì†Œìž¬ê°€ ì‹¤ì œë¡œ ê°€ìž¥ ìš°ìˆ˜í•  í™•ë¥ "**ê³¼ **"ê¸°ëŒ€ë˜ëŠ” ê°œì„ ëŸ‰"**ì„ ê³„ì‚°í•˜ì—¬ ì˜ˆì‚° ì¦ì•¡/ê°ì•¡ ë¹„ìœ¨ì„ ì œì•ˆí•©ë‹ˆë‹¤.
            """)
            st.divider()
            fig_scatter = px.scatter(res_agg, x='avg_cost_7d', y='exp_ctr', size='ë…¸ì¶œ', color='ID',
                                     labels={'avg_cost_7d': 'ìµœê·¼ 7ì¼ í‰ê·  ì§‘í–‰ ë¹„ìš©', 'exp_ctr': 'í†µê³„ì  ê¸°ëŒ€ CTR'},
                                     title="ì§‘í–‰ ë¹„ìš© ëŒ€ë¹„ ì„±ê³¼ í”„ë¡ í‹°ì–´ (ìš°ìƒë‹¨ ì†Œìž¬ê°€ ê³ íš¨ìœ¨)")
            st.plotly_chart(fig_scatter, use_container_width=True)
            
            if st.button("ìµœì  ì˜ˆì‚° ë°°ë¶„ ì •ì±… ì œì•ˆ ì‹¤í–‰"):
                res_agg['score'] = res_agg['exp_ctr'] * res_agg['prob_is_best']
                avg_s = res_agg['score'].mean() + 1e-9
                res_agg['proposed'] = res_agg['avg_cost_7d'] * (res_agg['score'] / avg_s)
                res_agg['ìµœì¢…ì œì•ˆì•¡'] = res_agg.apply(lambda r: np.clip(r['proposed'], r['avg_cost_7d']*0.7, r['avg_cost_7d']*1.3), axis=1)
                st.table(res_agg[['ID', 'exp_ctr', 'prob_is_best', 'ìµœì¢…ì œì•ˆì•¡']].style.format({'exp_ctr': '{:.4f}', 'prob_is_best': '{:.2f}', 'ìµœì¢…ì œì•ˆì•¡': '{:,.0f}'}))