import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
from scipy.stats import beta
from sklearn.linear_model import LinearRegression

st.set_page_config(page_title="ê²Œì„ ë§ˆì¼€íŒ… í†µí•© ë¶„ì„", layout="wide")
st.title("ğŸ® ê²Œì„ ë§ˆì¼€íŒ… í†µí•© ë¶„ì„ ì‹œìŠ¤í…œ")
st.markdown("**Bayesian í†µê³„ ê¸°ë°˜ ì„±ê³¼ ë¶„ì„ & ì˜ì‚¬ê²°ì • ì§€ì›**")
st.markdown("---")

def load_and_clean_data(uploaded_file):
    try:
        if uploaded_file.name.endswith('.xlsx'):
            all_sheets = pd.read_excel(uploaded_file, sheet_name=None)
            df = pd.concat(all_sheets.values(), ignore_index=True)
        else:
            df = pd.read_csv(uploaded_file, sep='\t' if uploaded_file.name.endswith('.tsv') else ',')
        
        df.columns = [c.strip() for c in df.columns]
        mapping = {
            'ë‚ ì§œ': ['ë‚ ì§œ', 'ì¼ì', 'date'], 
            'ë§¤ì²´': ['ë§¤ì²´', 'media'],
            'ìƒí’ˆ': ['ìƒí’ˆëª…', 'ìƒí’ˆ', 'product'], 
            'ì†Œì¬': ['ì†Œì¬ëª…', 'ì†Œì¬', 'material'],
            'ë…¸ì¶œ': ['ë…¸ì¶œìˆ˜', 'ë…¸ì¶œ', 'impressions'], 
            'í´ë¦­': ['í´ë¦­ìˆ˜', 'í´ë¦­', 'clicks'], 
            'ë¹„ìš©': ['ë¹„ìš©', 'ì§€ì¶œ', 'cost']
        }
        
        final_df = pd.DataFrame()
        for k, v in mapping.items():
            for col in v:
                if col in df.columns: 
                    final_df[k] = df[col]
                    break
        
        final_df['ë‚ ì§œ'] = pd.to_datetime(final_df['ë‚ ì§œ'], errors='coerce')
        for col in ['ë…¸ì¶œ', 'í´ë¦­', 'ë¹„ìš©']:
            final_df[col] = pd.to_numeric(
                final_df[col].astype(str).str.replace(r'[^\d.]', '', regex=True), 
                errors='coerce'
            ).fillna(0)
        
        final_df['CTR(%)'] = (final_df['í´ë¦­'] / (final_df['ë…¸ì¶œ'] + 1e-9) * 100)
        final_df['CPC'] = final_df['ë¹„ìš©'] / (final_df['í´ë¦­'] + 1e-9)
        final_df['ID'] = "[" + final_df['ìƒí’ˆ'].astype(str) + "] " + final_df['ì†Œì¬'].astype(str)
        
        return final_df.dropna(subset=['ë‚ ì§œ']).sort_values(['ID', 'ë‚ ì§œ'])
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def analyze_empirical_bayes(df, benchmark_df=None, use_manual_prior=False):
    global_ctr = df['í´ë¦­'].sum() / (df['ë…¸ì¶œ'].sum() + 1e-9)
    id_stats = df.groupby('ID').agg({'í´ë¦­': 'sum', 'ë…¸ì¶œ': 'sum', 'ë¹„ìš©': 'sum', 'ë§¤ì²´': 'first'})
    id_ctrs = id_stats['í´ë¦­'] / (id_stats['ë…¸ì¶œ'] + 1e-9)
    
    agg = id_stats.reset_index()
    agg['raw_ctr'] = id_ctrs.values
    
    if use_manual_prior and benchmark_df is not None:
        benchmark_dict = benchmark_df.set_index('ë§¤ì²´')['ì—…ê³„í‰ê· CTR(%)'].to_dict()
        strength_dict = benchmark_df.set_index('ë§¤ì²´')['Priorê°•ë„'].to_dict()
        
        for idx, row in agg.iterrows():
            media = row['ë§¤ì²´']
            if media in benchmark_dict:
                prior_ctr = benchmark_dict[media] / 100
                prior_strength = strength_dict[media]
                
                alpha_0 = prior_ctr * prior_strength
                beta_0 = (1 - prior_ctr) * prior_strength
            else:
                alpha_0, beta_0 = 1, 99
            
            agg.loc[idx, 'post_alpha'] = alpha_0 + row['í´ë¦­']
            agg.loc[idx, 'post_beta'] = beta_0 + (row['ë…¸ì¶œ'] - row['í´ë¦­'])
            agg.loc[idx, 'alpha_0'] = alpha_0
            agg.loc[idx, 'beta_0'] = beta_0
    else:
        var_ctr = max(id_ctrs.var(), 1e-7)
        kappa = (global_ctr * (1 - global_ctr) / var_ctr) - 1
        kappa = np.clip(kappa, 10, 1000)
        
        alpha_0, beta_0 = global_ctr * kappa, (1 - global_ctr) * kappa
        
        agg['post_alpha'] = alpha_0 + agg['í´ë¦­']
        agg['post_beta'] = beta_0 + (agg['ë…¸ì¶œ'] - agg['í´ë¦­'])
        agg['alpha_0'] = alpha_0
        agg['beta_0'] = beta_0
    
    agg['exp_ctr'] = agg['post_alpha'] / (agg['post_alpha'] + agg['post_beta'])
    
    samples = np.random.beta(
        agg['post_alpha'].values[:, None], 
        agg['post_beta'].values[:, None], 
        size=(len(agg), 5000)
    )
    agg['prob_is_best'] = np.bincount(
        np.argmax(samples, axis=0), 
        minlength=len(agg)
    ) / 5000
    
    max_date = df['ë‚ ì§œ'].max()
    date_7d_ago = max_date - timedelta(days=6)
    last_7d = df[df['ë‚ ì§œ'] >= date_7d_ago]
    last_costs = last_7d.groupby('ID')['ë¹„ìš©'].sum() / 7
    agg = agg.merge(last_costs.rename('avg_cost_7d'), on='ID', how='left').fillna(0)
    
    return agg

def get_binomial_cusum(clicks, imps, p0):
    p1 = np.clip(p0 * 0.85, 1e-6, 1-1e-6)
    p0 = np.clip(p0, 1e-6, 1-1e-6)
    llr = clicks * np.log(p1/p0) + (imps - clicks) * np.log((1-p1)/(1-p0))
    s = 0
    cusum = []
    for val in llr:
        s = min(0, s + val)
        cusum.append(s)
    return np.array(cusum)

@st.cache_data
def estimate_h_via_arl(p0, imps_series, target_arl=30, sims=500):
    p1 = np.clip(p0 * 0.85, 1e-6, 1-1e-6)
    p0_clip = np.clip(p0, 1e-6, 1-1e-6)
    llr_success = np.log(p1 / p0_clip)
    llr_failure = np.log((1 - p1) / (1 - p0_clip))
    
    h_candidates = np.arange(1.0, 30.0, 0.5)
    
    for h in h_candidates:
        run_lengths = []
        for _ in range(sims):
            s, t = 0, 0
            while t < 500:
                t += 1
                n = np.random.choice(imps_series) if len(imps_series) > 0 else 100000
                c = np.random.binomial(int(n), p0_clip)
                s = min(0, s + (c * llr_success + (int(n) - c) * llr_failure))
                if s < -h:
                    break
            run_lengths.append(t)
        
        actual_arl = np.mean(run_lengths)
        if actual_arl >= target_arl:
            return h, actual_arl
    
    return h_candidates[-1], np.mean(run_lengths)

def get_confidence_level(material, df):
    mat_id = material['ID']
    mat_data = df[df['ID'] == mat_id]
    
    data_score = 1 if material['ë…¸ì¶œ'] > 1000000 else (0.5 if material['ë…¸ì¶œ'] > 100000 else 0)
    
    if len(mat_data) >= 7:
        daily_ctr_std = mat_data['CTR(%)'].std()
        stability_score = 1 if daily_ctr_std < material['exp_ctr'] * 50 else (0.5 if daily_ctr_std < material['exp_ctr'] * 100 else 0)
    else:
        stability_score = 0
    
    total_score = (data_score + stability_score) / 2
    
    if total_score >= 0.7:
        return "ğŸŸ¢ ë†’ìŒ", "ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì•ˆì •ì  íŒ¨í„´"
    elif total_score >= 0.4:
        return "ğŸŸ¡ ë³´í†µ", "ì¶”ê°€ ê´€ì°° ê¶Œì¥"
    else:
        return "ğŸ”´ ë‚®ìŒ", "ë°ì´í„° ë¶€ì¡± ë˜ëŠ” ë³€ë™ì„± ë†’ìŒ"

with st.sidebar:
    st.markdown("## âš™ï¸ ë¶„ì„ ì„¤ì •")
    
    st.markdown("### ğŸ“Š Prior ì„¤ì • ë°©ì‹")
    prior_mode = st.radio(
        "Prior ì„¤ì •",
        ["ìë™ (ë°ì´í„° ê¸°ë°˜)", "ìˆ˜ë™ (ë²¤ì¹˜ë§ˆí¬ ê¸°ë°˜)"],
        help="ìë™: í˜„ì¬ ë°ì´í„°ë¡œ Prior ì¶”ì • / ìˆ˜ë™: ì—…ê³„ ë²¤ì¹˜ë§ˆí¬ ì…ë ¥"
    )
    
    benchmark_df = None
    if prior_mode == "ìˆ˜ë™ (ë²¤ì¹˜ë§ˆí¬ ê¸°ë°˜)":
        st.markdown("### ğŸ“‹ ìƒí’ˆë³„ ë²¤ì¹˜ë§ˆí¬ ì…ë ¥")
        
        if 'benchmark_data' not in st.session_state:
            st.session_state.benchmark_data = pd.DataFrame({
                'ë§¤ì²´': ['ë„¤ì´ë²„ GFA', 'ìœ íŠœë¸Œ', 'GDN', 'í˜ì´ìŠ¤ë¶'],
                'ì—…ê³„í‰ê· CTR(%)': [0.8, 2.5, 0.3, 1.2],
                'Priorê°•ë„': [100, 100, 100, 100]
            })
        
        edited_benchmark = st.data_editor(
            st.session_state.benchmark_data,
            num_rows="dynamic",
            use_container_width=True,
            column_config={
                'ë§¤ì²´': st.column_config.TextColumn("ë§¤ì²´ëª…", help="ë°ì´í„°ì˜ 'ë§¤ì²´' ì»¬ëŸ¼ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨"),
                'ì—…ê³„í‰ê· CTR(%)': st.column_config.NumberColumn("ì—…ê³„ í‰ê·  CTR (%)", min_value=0.0, max_value=10.0, format="%.2f"),
                'Priorê°•ë„': st.column_config.NumberColumn("Prior ê°•ë„", min_value=10, max_value=1000, help="ë†’ì„ìˆ˜ë¡ ë²¤ì¹˜ë§ˆí¬ ì˜ì¡´ë„ ì¦ê°€")
            }
        )
        st.session_state.benchmark_data = edited_benchmark
        benchmark_df = edited_benchmark
        
        with st.expander("â„¹ï¸ Prior ê°•ë„ë€?"):
            st.markdown("""
            **Prior ê°•ë„ = ê°€ìƒì˜ "ê³¼ê±° ë°ì´í„°" ì–‘**
            
            - **100**: ë²¤ì¹˜ë§ˆí¬ CTRë¡œ ë…¸ì¶œ 10ë§ŒíšŒ ë³¸ ê²ƒì²˜ëŸ¼ ì·¨ê¸‰
            - **500**: ë…¸ì¶œ 50ë§ŒíšŒ (ë²¤ì¹˜ë§ˆí¬ ê°•í•˜ê²Œ ì‹ ë¢°)
            - **10**: ë…¸ì¶œ 1ë§ŒíšŒ (ì‹¤ì œ ë°ì´í„°ì— ë¹ ë¥´ê²Œ ì ì‘)
            
            **ê¶Œì¥:**
            - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì—…ê³„ ë°ì´í„°: 200-500
            - ëŒ€ëµì  ì¶”ì •ì¹˜: 50-100
            - ë°ì´í„° 2ì£¼ì¹˜ë§Œ ìˆìœ¼ë©´: 100 ê¶Œì¥
            """)

uploaded_file = st.file_uploader("ğŸ“‚ ìº í˜ì¸ ë°ì´í„° ì—…ë¡œë“œ (CSV/XLSX/TSV)", type=['csv', 'xlsx', 'tsv'])

if uploaded_file:
    df = load_and_clean_data(uploaded_file)
    
    if not df.empty:
        use_manual_prior = (prior_mode == "ìˆ˜ë™ (ë²¤ì¹˜ë§ˆí¬ ê¸°ë°˜)")
        res_agg = analyze_empirical_bayes(df, benchmark_df, use_manual_prior)
        ids = sorted(df['ID'].unique())
        
        st.markdown("---")
        
        tabs = st.tabs([
            "ğŸ“‹ ì£¼ê°„ ì²´í¬ë¦¬ìŠ¤íŠ¸", 
            "ğŸ“Š ì„±ê³¼ ëŒ€ì‹œë³´ë“œ", 
            "ğŸ§¬ Bayesian ë¶„ì„",
            "â° ì¡°ê¸° ê²½ê³ ",
            "ğŸ“‰ CUSUM ëª¨ë‹ˆí„°ë§"
        ])
        
        with tabs[0]:
            st.markdown("## ğŸ“‹ ì£¼ê°„ ì˜ì‚¬ê²°ì • ì²´í¬ë¦¬ìŠ¤íŠ¸")
            st.markdown(f"**ë¶„ì„ ê¸°ì¤€ì¼: {df['ë‚ ì§œ'].max().strftime('%Yë…„ %mì›” %dì¼')}**")
            st.markdown("---")
            
            today = df['ë‚ ì§œ'].max()
            this_week_start = today - timedelta(days=6)
            last_week_start = this_week_start - timedelta(days=7)
            last_week_end = this_week_start - timedelta(days=1)
            
            this_week = df[df['ë‚ ì§œ'] >= this_week_start]
            last_week = df[(df['ë‚ ì§œ'] >= last_week_start) & (df['ë‚ ì§œ'] <= last_week_end)]
            
            st.markdown("### ğŸš¨ ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”")
            
            critical_items = []
            
            for material in res_agg.iterrows():
                _, mat = material
                mat_id = mat['ID']
                
                mat_this_week = this_week[this_week['ID'] == mat_id]['CTR(%)'].mean()
                mat_last_week = last_week[last_week['ID'] == mat_id]['CTR(%)'].mean()
                
                if mat_last_week > 0:
                    change_pct = (mat_this_week - mat_last_week) / mat_last_week
                    if change_pct < -0.3:
                        critical_items.append({
                            'ì†Œì¬': mat_id,
                            'ë¬¸ì œ': f'CTR {abs(change_pct)*100:.0f}% ê¸‰ë½',
                            'ì´ë²ˆì£¼': f'{mat_this_week:.2f}%',
                            'ì§€ë‚œì£¼': f'{mat_last_week:.2f}%',
                            'ì•¡ì…˜': 'ì†Œì¬ êµì²´ ë˜ëŠ” íƒ€ê²Ÿ ì¬ì„¤ì •',
                            'ìš°ì„ ìˆœìœ„': 1
                        })
                
                mat_cost = this_week[this_week['ID'] == mat_id]['ë¹„ìš©'].sum()
                total_cost = this_week['ë¹„ìš©'].sum()
                cost_share = mat_cost / total_cost if total_cost > 0 else 0
                
                mat_clicks = this_week[this_week['ID'] == mat_id]['í´ë¦­'].sum()
                total_clicks = this_week['í´ë¦­'].sum()
                click_share = mat_clicks / total_clicks if total_clicks > 0 else 0
                
                if cost_share > 0.4 and click_share < 0.3:
                    critical_items.append({
                        'ì†Œì¬': mat_id,
                        'ë¬¸ì œ': f'ë¹„ìš© {cost_share*100:.0f}%, í´ë¦­ {click_share*100:.0f}%',
                        'ì´ë²ˆì£¼': f'{mat_cost:,.0f}ì›',
                        'ì§€ë‚œì£¼': '-',
                        'ì•¡ì…˜': 'ì˜ˆì‚° ì¬ë¶„ë°° ë˜ëŠ” ì…ì°°ê°€ ì¡°ì •',
                        'ìš°ì„ ìˆœìœ„': 1
                    })
            
            if len(critical_items) > 0:
                st.error(f"âš ï¸ {len(critical_items)}ê±´ì˜ ê¸´ê¸‰ ì´ìŠˆ")
                for idx, item in enumerate(critical_items, 1):
                    with st.expander(f"ğŸ”´ [{idx}] {item['ì†Œì¬']}: {item['ë¬¸ì œ']}", expanded=True):
                        col1, col2 = st.columns(2)
                        col1.metric("ì´ë²ˆì£¼", item['ì´ë²ˆì£¼'])
                        col2.metric("ì§€ë‚œì£¼", item['ì§€ë‚œì£¼'])
                        st.warning(f"**ê¶Œì¥ ì•¡ì…˜:** {item['ì•¡ì…˜']}")
            else:
                st.success("âœ… ê¸´ê¸‰ ì¡°ì¹˜ í•„ìš”í•œ í•­ëª© ì—†ìŒ")
            
            st.markdown("---")
            st.markdown("### ğŸ’¡ ê°œì„  ê¸°íšŒ")
            
            opportunities = []
            
            material_perf = this_week.groupby('ID').agg({
                'CTR(%)': 'mean',
                'ë¹„ìš©': 'sum',
                'í´ë¦­': 'sum'
            }).reset_index()
            
            if len(material_perf) > 0:
                best_ctr = material_perf.loc[material_perf['CTR(%)'].idxmax()]
                if best_ctr['ë¹„ìš©'] / this_week['ë¹„ìš©'].sum() < 0.4:
                    opportunities.append({
                        'ê¸°íšŒ': f"ğŸŸ¢ ê³ ì„±ê³¼ ì†Œì¬ '{best_ctr['ID']}' ì¦ì•¡ ê¸°íšŒ",
                        'ê·¼ê±°': f"CTR {best_ctr['CTR(%)']:.2f}%ë¡œ 1ìœ„, ì˜ˆì‚° ì ìœ ìœ¨ {best_ctr['ë¹„ìš©']/this_week['ë¹„ìš©'].sum()*100:.0f}%",
                        'ì œì•ˆ': "10-20% ì ì§„ ì¦ì•¡ í›„ 3ì¼ ëª¨ë‹ˆí„°ë§"
                    })
            
            media_diversity = this_week.groupby('ë§¤ì²´')['ë¹„ìš©'].sum()
            if len(media_diversity) > 0 and (media_diversity / media_diversity.sum()).max() > 0.6:
                opportunities.append({
                    'ê¸°íšŒ': f"ğŸ“± ë§¤ì²´ ë‹¤ê°í™” í•„ìš” ({media_diversity.idxmax()} {media_diversity.max()/media_diversity.sum()*100:.0f}%)",
                    'ê·¼ê±°': "ë‹¨ì¼ ë§¤ì²´ ì˜ì¡´ë„ ë†’ìŒ",
                    'ì œì•ˆ': "íƒ€ ë§¤ì²´ ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹œì‘"
                })
            
            if len(opportunities) > 0:
                for idx, opp in enumerate(opportunities, 1):
                    with st.expander(f"ğŸ’¡ [{idx}] {opp['ê¸°íšŒ']}", expanded=False):
                        st.info(f"**ê·¼ê±°:** {opp['ê·¼ê±°']}")
                        st.success(f"**ì œì•ˆ:** {opp['ì œì•ˆ']}")
            else:
                st.info("ì¶”ê°€ ê°œì„  ê¸°íšŒ ì—†ìŒ (í˜„ìƒ ìœ ì§€)")
            
            st.markdown("---")
            st.markdown("### ğŸ“Š ì´ë²ˆì£¼ ì„±ê³¼ ìš”ì•½")
            
            col1, col2, col3, col4 = st.columns(4)
            
            this_week_cost = this_week['ë¹„ìš©'].sum()
            last_week_cost = last_week['ë¹„ìš©'].sum()
            cost_change = (this_week_cost - last_week_cost) / last_week_cost if last_week_cost > 0 else 0
            
            this_week_clicks = this_week['í´ë¦­'].sum()
            last_week_clicks = last_week['í´ë¦­'].sum()
            clicks_change = (this_week_clicks - last_week_clicks) / last_week_clicks if last_week_clicks > 0 else 0
            
            this_week_ctr = (this_week['í´ë¦­'].sum() / this_week['ë…¸ì¶œ'].sum()) * 100
            last_week_ctr = (last_week['í´ë¦­'].sum() / last_week['ë…¸ì¶œ'].sum()) * 100
            ctr_change = this_week_ctr - last_week_ctr
            
            this_week_cpc = this_week_cost / this_week_clicks if this_week_clicks > 0 else 0
            last_week_cpc = last_week_cost / last_week_clicks if last_week_clicks > 0 else 0
            cpc_change = this_week_cpc - last_week_cpc
            
            col1.metric("ì´ ì§€ì¶œ", f"{this_week_cost:,.0f}ì›", f"{cost_change*100:+.1f}%")
            col2.metric("ì´ í´ë¦­", f"{this_week_clicks:,}íšŒ", f"{clicks_change*100:+.1f}%")
            col3.metric("í‰ê·  CTR", f"{this_week_ctr:.2f}%", f"{ctr_change:+.2f}%p")
            col4.metric("í‰ê·  CPC", f"{this_week_cpc:,.0f}ì›", f"{cpc_change:+.0f}ì›")
        
        with tabs[1]:
            st.markdown("### ğŸ“Š ì„±ê³¼ ëŒ€ì‹œë³´ë“œ")
            
            col1, col2, col3, col4 = st.columns(4)
            global_ctr = df['í´ë¦­'].sum() / (df['ë…¸ì¶œ'].sum() + 1e-9)
            col1.metric("ì „ì²´ í‰ê·  CTR", f"{global_ctr*100:.2f}%")
            col2.metric("ë¶„ì„ ê¸°ê°„", f"{(df['ë‚ ì§œ'].max() - df['ë‚ ì§œ'].min()).days}ì¼")
            col3.metric("ì´ ì†Œì¬ ìˆ˜", len(ids))
            col4.metric("ì´ ì§‘í–‰ ë¹„ìš©", f"{df['ë¹„ìš©'].sum():,.0f}ì›")
            
            st.markdown("---")
            st.markdown("### ğŸ† ì†Œì¬ë³„ ìµœê³  ì„±ê³¼ í™•ë¥ ")
            
            fig_prob = px.bar(
                res_agg.sort_values('prob_is_best', ascending=True),
                x='prob_is_best', y='ID', orientation='h',
                text=res_agg.sort_values('prob_is_best', ascending=True)['prob_is_best'].apply(lambda x: f"{x*100:.1f}%")
            )
            fig_prob.update_xaxes(tickformat='.0%', title='ìµœê³  ì„±ê³¼ í™•ë¥ ')
            fig_prob.update_yaxes(title='')
            fig_prob.update_traces(textposition='outside')
            st.plotly_chart(fig_prob, use_container_width=True)
            
            st.markdown("---")
            st.markdown("### ğŸ“ˆ ì†Œì¬ë³„ ìƒì„¸ ì„±ê³¼")
            
            display_df = res_agg[['ID', 'raw_ctr', 'exp_ctr', 'ë…¸ì¶œ', 'í´ë¦­', 'ë¹„ìš©', 'prob_is_best', 'avg_cost_7d']].copy()
            display_df['raw_ctr'] = display_df['raw_ctr'] * 100
            display_df['exp_ctr'] = display_df['exp_ctr'] * 100
            display_df['prob_is_best'] = display_df['prob_is_best'] * 100
            display_df.columns = ['ì†Œì¬', 'ì›ë³¸CTR(%)', 'ë³´ì •CTR(%)', 'ë…¸ì¶œìˆ˜', 'í´ë¦­ìˆ˜', 'ë¹„ìš©', 'ìµœê³ í™•ë¥ (%)', 'ì¼í‰ê· ë¹„ìš©']
            
            st.dataframe(
                display_df.style.format({
                    'ì›ë³¸CTR(%)': '{:.2f}',
                    'ë³´ì •CTR(%)': '{:.2f}',
                    'ë…¸ì¶œìˆ˜': '{:,.0f}',
                    'í´ë¦­ìˆ˜': '{:,.0f}',
                    'ë¹„ìš©': '{:,.0f}',
                    'ìµœê³ í™•ë¥ (%)': '{:.1f}',
                    'ì¼í‰ê· ë¹„ìš©': '{:,.0f}'
                }).background_gradient(subset=['ë³´ì •CTR(%)'], cmap='RdYlGn'),
                use_container_width=True
            )
            
            st.markdown("---")
            st.markdown("### ğŸ“Š CTR ì¶”ì´")
            
            daily_ctr = df.groupby(['ë‚ ì§œ', 'ID']).agg({
                'í´ë¦­': 'sum',
                'ë…¸ì¶œ': 'sum'
            }).reset_index()
            daily_ctr['CTR'] = (daily_ctr['í´ë¦­'] / daily_ctr['ë…¸ì¶œ']) * 100
            
            fig = px.line(daily_ctr, x='ë‚ ì§œ', y='CTR', color='ID', markers=True)
            fig.update_layout(yaxis_title='CTR (%)', xaxis_title='')
            st.plotly_chart(fig, use_container_width=True)
        
        with tabs[2]:
            st.markdown("### ğŸ§¬ Bayesian ë¶„ì„ ìƒì„¸")
            
            st.markdown("#### Prior ì„¤ì • í˜„í™©")
            
            if use_manual_prior:
                st.success("âœ… ìˆ˜ë™ ì„¤ì • ëª¨ë“œ (ë²¤ì¹˜ë§ˆí¬ ê¸°ë°˜)")
                
                prior_summary = res_agg[['ID', 'ë§¤ì²´', 'alpha_0', 'beta_0']].copy()
                prior_summary['Prior_CTR(%)'] = (prior_summary['alpha_0'] / (prior_summary['alpha_0'] + prior_summary['beta_0'])) * 100
                prior_summary['Prior_ê°•ë„'] = prior_summary['alpha_0'] + prior_summary['beta_0']
                
                st.dataframe(
                    prior_summary[['ID', 'ë§¤ì²´', 'Prior_CTR(%)', 'Prior_ê°•ë„']].style.format({
                        'Prior_CTR(%)': '{:.2f}',
                        'Prior_ê°•ë„': '{:.0f}'
                    }),
                    use_container_width=True
                )
            else:
                st.info("â„¹ï¸ ìë™ ì„¤ì • ëª¨ë“œ (ë°ì´í„° ê¸°ë°˜)")
                
                alpha_0 = res_agg['alpha_0'].iloc[0]
                beta_0 = res_agg['beta_0'].iloc[0]
                kappa = alpha_0 + beta_0
                prior_ctr = alpha_0 / (alpha_0 + beta_0)
                
                col1, col2, col3 = st.columns(3)
                col1.metric("Prior Î±â‚€", f"{alpha_0:.1f}")
                col2.metric("Prior Î²â‚€", f"{beta_0:.1f}")
                col3.metric("Îº (Kappa)", f"{kappa:.1f}")
                
                st.markdown(f"""
                **Prior CTR:** {prior_ctr*100:.2f}%  
                **Prior ê°•ë„(Îº):** {kappa:.1f} (ê°€ìƒ ë…¸ì¶œ {kappa*10000:,.0f}íšŒ ìƒë‹¹)
                """)
            
            st.markdown("---")
            st.markdown("#### Posterior ë¶„í¬ (ì‹¤ì œ CTR ì¶”ì •)")
            
            fig_post = go.Figure()
            colors = px.colors.qualitative.Set2
            
            for idx, (_, row) in enumerate(res_agg.iterrows()):
                x = np.linspace(0, 0.05, 500)
                y = beta.pdf(x, row['post_alpha'], row['post_beta'])
                fig_post.add_trace(go.Scatter(
                    x=x*100, y=y, name=row['ID'],
                    mode='lines', fill='tozeroy', opacity=0.6,
                    line=dict(color=colors[idx % len(colors)], width=3)
                ))
            
            fig_post.update_layout(
                title="ê° ì†Œì¬ì˜ ì‹¤ì œ CTR ë¶„í¬ ì¶”ì • (Posterior Distribution)",
                xaxis_title="CTR (%)",
                yaxis_title="í™•ë¥  ë°€ë„",
                height=500
            )
            st.plotly_chart(fig_post, use_container_width=True)
            
            st.markdown("---")
            st.markdown("#### ì‹ ë¢°ë„ í‰ê°€")
            
            conf_data = []
            for _, material in res_agg.iterrows():
                conf_level, conf_reason = get_confidence_level(material, df)
                conf_data.append({
                    'ì†Œì¬': material['ID'],
                    'ì‹ ë¢°ë„': conf_level,
                    'ì´ìœ ': conf_reason,
                    'ë…¸ì¶œìˆ˜': material['ë…¸ì¶œ'],
                    'ë°ì´í„°ì¼ìˆ˜': len(df[df['ID'] == material['ID']])
                })
            
            conf_df = pd.DataFrame(conf_data)
            st.dataframe(
                conf_df.style.format({'ë…¸ì¶œìˆ˜': '{:,.0f}'}),
                use_container_width=True
            )
        
        with tabs[3]:
            st.markdown("### â° ì†Œì¬ í”¼ë¡œë„ ì¡°ê¸° ê²½ê³ ")
            
            st.markdown("""
            **ì†Œì¬ í”¼ë¡œë„(Creative Fatigue):** ë™ì¼ ì†Œì¬ ë°˜ë³µ ë…¸ì¶œ ì‹œ CTR í•˜ë½ í˜„ìƒ  
            ì„ í˜• íšŒê·€ë¡œ ì¶”ì„¸ë¥¼ ë¶„ì„í•˜ì—¬ êµì²´ ì‹œì ì„ ì¡°ê¸° ì˜ˆì¸¡í•©ë‹ˆë‹¤.
            """)
            
            st.markdown("---")
            
            for mat_id in ids:
                mat_data = df[df['ID'] == mat_id].sort_values('ë‚ ì§œ')
                
                if len(mat_data) < 5:
                    st.warning(f"**{mat_id}**: ë°ì´í„° ë¶€ì¡± (ìµœì†Œ 5ì¼ í•„ìš”)")
                    continue
                
                X = np.arange(len(mat_data)).reshape(-1, 1)
                y = mat_data['CTR(%)'].values
                model = LinearRegression().fit(X, y)
                slope = model.coef_[0]
                current_ctr = y[-1]
                
                if slope < -0.001:
                    days_left = max(0, int((current_ctr - current_ctr * 0.5) / abs(slope)))
                    
                    if days_left == 0:
                        lifespan_status = "âš ï¸ ì¦‰ì‹œ êµì²´ ê²€í† "
                    elif days_left <= 3:
                        lifespan_status = f"ğŸ”´ ê¸´ê¸‰ (ì¶”ì • D-{days_left})"
                    elif days_left <= 7:
                        lifespan_status = f"ğŸŸ¡ ì£¼ì˜ (ì¶”ì • D-{days_left})"
                    else:
                        lifespan_status = f"ğŸŸ¢ ì•ˆì • (ì¶”ì • D-{days_left})"
                else:
                    lifespan_status = "âœ… í•˜ë½ ì¶”ì„¸ ì—†ìŒ"
                    days_left = None
                
                col1, col2 = st.columns([2, 1])
                with col1:
                    st.markdown(f"**{mat_id}**")
                    st.markdown(f"**ìƒíƒœ:** {lifespan_status}")
                    st.markdown(f"í˜„ì¬ CTR: {current_ctr:.2f}% | ì¼í‰ê·  ë³€í™”: {slope:.4f}%p")
                    if days_left is not None and days_left > 0:
                        st.caption("â€» ì„ í˜• ì¶”ì„¸ ê¸°ì¤€ ì°¸ê³  ì¶”ì •ì¹˜")
                
                with col2:
                    fig_mini = go.Figure()
                    fig_mini.add_trace(go.Scatter(
                        x=mat_data['ë‚ ì§œ'], y=y, 
                        mode='lines+markers', name='ì‹¤ì œ'
                    ))
                    trend_line = model.predict(X)
                    fig_mini.add_trace(go.Scatter(
                        x=mat_data['ë‚ ì§œ'], y=trend_line,
                        mode='lines', name='ì¶”ì„¸', 
                        line=dict(dash='dash', color='red')
                    ))
                    fig_mini.update_layout(
                        height=200, showlegend=False,
                        margin=dict(l=0, r=0, t=0, b=0),
                        yaxis_title='CTR(%)'
                    )
                    st.plotly_chart(fig_mini, use_container_width=True)
                
                st.markdown("---")
        
        with tabs[4]:
            st.markdown("### ğŸ“‰ CUSUM ì´ìƒ ê°ì§€")
            
            st.markdown("""
            **CUSUM(Cumulative Sum):** í†µê³„ì  ê³µì • ê´€ë¦¬ ê¸°ë²•  
            ê¸°ì¤€ ì„±ê³¼ ëŒ€ë¹„ ëˆ„ì  ì´íƒˆë„ë¥¼ ì¶”ì í•˜ì—¬ ì„±ê³¼ í•˜ë½ì„ ì¡°ê¸° ê°ì§€í•©ë‹ˆë‹¤.
            """)
            
            st.markdown("---")
            
            selected_material = st.selectbox("ì†Œì¬ ì„ íƒ", ids)
            sub = df[df['ID'] == selected_material].sort_values('ë‚ ì§œ')
            
            if len(sub) >= 7:
                p0_val = sub.head(7)['í´ë¦­'].sum() / (sub.head(7)['ë…¸ì¶œ'].sum() + 1e-9)
            else:
                p0_val = sub['í´ë¦­'].sum() / (sub['ë…¸ì¶œ'].sum() + 1e-9)
            
            cusum_vals = get_binomial_cusum(sub['í´ë¦­'].values, sub['ë…¸ì¶œ'].values, p0_val)
            h_threshold, achieved_arl = estimate_h_via_arl(p0_val, sub['ë…¸ì¶œ'].values, sims=200)
            h_threshold = -h_threshold
            
            col1, col2, col3 = st.columns(3)
            col1.metric("ê¸°ì¤€ CTR (p0)", f"{p0_val*100:.2f}%")
            col2.metric("ê°ì§€ ì„ê³„ê°’ (h)", f"{h_threshold:.2f}")
            col3.metric("í˜„ì¬ CUSUM", f"{cusum_vals[-1]:.2f}")
            
            fig_cusum = go.Figure()
            fig_cusum.add_trace(go.Scatter(
                x=sub['ë‚ ì§œ'], y=cusum_vals,
                mode='lines+markers', name='CUSUM',
                line=dict(color='blue', width=2)
            ))
            fig_cusum.add_hline(
                y=h_threshold, line_dash="dash",
                line_color="red", annotation_text="ì„ê³„ê°’"
            )
            fig_cusum.update_layout(
                title=f"{selected_material} - CUSUM ëª¨ë‹ˆí„°ë§",
                xaxis_title="ë‚ ì§œ",
                yaxis_title="CUSUM ê°’",
                height=400
            )
            st.plotly_chart(fig_cusum, use_container_width=True)
            
            if cusum_vals[-1] < h_threshold:
                st.error(f"âš ï¸ **ì„±ê³¼ í•˜ë½ ê°ì§€** (CUSUM: {cusum_vals[-1]:.2f} < ì„ê³„ê°’: {h_threshold:.2f})")
                st.markdown("""
                **ê¶Œì¥ ì¡°ì¹˜:**
                - ì†Œì¬ ì¦‰ì‹œ êµì²´ ê²€í† 
                - íƒ€ê²ŸíŒ… ì„¤ì • ì¬í™•ì¸
                - ê²½ìŸì‚¬ ë™í–¥ ë¶„ì„
                """)
            else:
                st.success(f"âœ… **ì •ìƒ ë²”ìœ„** (CUSUM: {cusum_vals[-1]:.2f})")
            
            with st.expander("â„¹ï¸ CUSUM í•´ì„ ê°€ì´ë“œ"):
                st.markdown("""
                **CUSUM ê°’ì˜ ì˜ë¯¸:**
                - **0 ë¶€ê·¼:** ê¸°ì¤€ ì„±ê³¼ ëŒ€ë¹„ ì •ìƒ ë²”ìœ„
                - **ìŒìˆ˜ ì¦ê°€:** ì„±ê³¼ê°€ ì§€ì†ì ìœ¼ë¡œ í•˜ë½ ì¤‘
                - **ì„ê³„ê°’ ëŒíŒŒ:** í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ í•˜ë½ ê°ì§€
                
                **ì¥ì :**
                - ì‘ì€ ë³€í™”ë„ ëˆ„ì í•˜ì—¬ ì¡°ê¸° ê°ì§€
                - ì¼ì‹œì  ë³€ë™ê³¼ êµ¬ì¡°ì  í•˜ë½ êµ¬ë¶„
                
                **í•œê³„:**
                - ì™¸ë¶€ ìš”ì¸(ì‹œì¦Œ, ê²½ìŸì‚¬) ë¯¸ë°˜ì˜
                - ìƒìŠ¹ ì „í™˜ ê°ì§€ëŠ” ë³„ë„ ì„¤ì • í•„ìš”
                """)
        
        st.markdown("---")
        
        with st.expander("ğŸ” í˜„ì¬ ë°ì´í„°ë¡œ ë‹µí•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸", expanded=False):
            st.markdown("""
            ### âŒ í˜„ì¬ ë°ì´í„°ì˜ í•œê³„
            
            **1. ì „í™˜ ì„±ê³¼ ë¶„ì„ ë¶ˆê°€**
            - ì§ˆë¬¸: "CTR ë†’ì€ ì†Œì¬ê°€ ì‹¤ì œ Install/ë§¤ì¶œ ê¸°ì—¬í•˜ëŠ”ê°€?"
            - í•„ìš” ë°ì´í„°: Install, íšŒì›ê°€ì…, ì¸ì•± ê²°ì œ ì „í™˜ ë°ì´í„°
            - ì˜í–¥: CTRë§Œìœ¼ë¡œ íŒë‹¨ ì‹œ CPIê°€ ë†’ì€ ë¹„íš¨ìœ¨ì  ì†Œì¬ ì„ íƒ ìœ„í—˜
            
            **2. ì¸ê³¼ ê´€ê³„ ì¶”ì • ë¶ˆê°€**
            - ì§ˆë¬¸: "ì˜ˆì‚° 2ë°° ì¦ì•¡ ì‹œ Install ëª‡ ê°œ ì¦ê°€?"
            - í•„ìš” ë°ì´í„°: ê³¼ê±° ì˜ˆì‚° ë³€ê²½ ì‹¤í—˜ ë°ì´í„° (A/B í…ŒìŠ¤íŠ¸)
            - ì˜í–¥: ì„ í˜• ê°€ì •ë§Œ ê°€ëŠ¥, ì‹¤ì œë¡  ë¹„ì„ í˜• ë°˜ì‘
            
            **3. íƒ€ê²Ÿ ìµœì í™” ì œí•œ**
            - ì§ˆë¬¸: "ì–´ë–¤ ìœ ì € ì„¸ê·¸ë¨¼íŠ¸ê°€ ì „í™˜ìœ¨ ë†’ì€ê°€?"
            - í•„ìš” ë°ì´í„°: ì—°ë ¹/ì„±ë³„/ê´€ì‹¬ì‚¬ë³„ ì„±ê³¼ ë¶„í•´
            - ì˜í–¥: ê´‘ë²”ìœ„ íƒ€ê²ŸíŒ…ë§Œ ê°€ëŠ¥, ì •ë°€ ìµœì í™” ë¶ˆê°€
            
            **4. ì¥ê¸° ì˜ˆì¸¡ ë¶ˆê°€**
            - ì§ˆë¬¸: "ì´ ì†Œì¬ê°€ 3ê°œì›” í›„ì—ë„ ì„±ê³¼ ìœ ì§€?"
            - í•„ìš” ë°ì´í„°: ìµœì†Œ 3-6ê°œì›” ì´ìƒì˜ ì¥ê¸° ì¶”ì  ë°ì´í„°
            - ì˜í–¥: 2ì£¼ ë°ì´í„°ë¡œëŠ” ì¶”ì„¸ë§Œ íŒŒì•…, ì˜ˆì¸¡ ì‹ ë¢°ë„ ë‚®ìŒ
            
            ---
            
            ### âœ… í˜„ì¬ ë°ì´í„°ë¡œ ë‹µí•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸
            
            **1. ì¡°ê¸° ê²½ê³ **
            - ì–´ë–¤ ì†Œì¬ê°€ ì„±ê³¼ í•˜ë½ ì¤‘ì¸ê°€? (CUSUM, ì„ í˜• íšŒê·€)
            - ì–¸ì œ ì†Œì¬ë¥¼ êµì²´í•´ì•¼ í•˜ëŠ”ê°€? (í”¼ë¡œë„ ì¶”ì •)
            
            **2. íš¨ìœ¨ì„± ë¹„êµ**
            - ì˜ˆì‚°ì´ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ë°°ë˜ê³  ìˆë‚˜? (ë¹„ìš©/í´ë¦­ ë¹„ìœ¨)
            - ë§¤ì²´ë³„/ìƒí’ˆë³„ ì„±ê³¼ ì°¨ì´ëŠ”? (CTR, CPC ë¹„êµ)
            
            **3. í†µê³„ì  ìš°ì—´ íŒë‹¨**
            - ì†Œì¬ Aì™€ B ì¤‘ ì–´ëŠ ìª½ì´ í†µê³„ì ìœ¼ë¡œ ìš°ìˆ˜í•œê°€? (Bayesian)
            - ìš°ì—°ì¸ê°€ ì‹¤ë ¥ì¸ê°€? (ì‹ ë¢° êµ¬ê°„)
            
            **4. ë‹¨ê¸° ì˜ì‚¬ê²°ì •**
            - ë‚´ì¼/ì´ë²ˆì£¼ ì–´ë–¤ ì•¡ì…˜ì„ ì·¨í•´ì•¼ í•˜ë‚˜? (ì²´í¬ë¦¬ìŠ¤íŠ¸)
            - ì–´ë–¤ ì†Œì¬ì— ìš°ì„  ì˜ˆì‚°ì„ ë°°ë¶„í• ê¹Œ? (ìµœê³  í™•ë¥ )
            
            ---
            
            **â†’ ì´ ì‹œìŠ¤í…œì˜ í¬ì§€ì…”ë‹:**  
            "ì™„ë²½í•œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"ì´ ì•„ë‹Œ **"ì§€ê¸ˆ ë‹¹ì¥ ì¡°ì¹˜ í•„ìš”í•œ ê²ƒì„ ì°¾ëŠ” ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ"**
            """)
    else:
        st.warning("ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    st.info("ğŸ‘† ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”")
    
    st.markdown("---")
    st.markdown("### ğŸ“‹ ì‹œìŠ¤í…œ ê¸°ëŠ¥ ì†Œê°œ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        #### âœ¨ í•µì‹¬ ê¸°ëŠ¥
        
        **1. ë²¤ì¹˜ë§ˆí¬ ê¸°ë°˜ Prior ì„¤ì •**
        - ë§¤ì²´ë³„ ì—…ê³„ í‰ê·  CTR ì…ë ¥
        - Prior ê°•ë„ ì¡°ì • (10~1000)
        - ì†ŒëŸ‰ ë°ì´í„°ì—ì„œë„ ì•ˆì •ì  ì¶”ì •
        
        **2. ì£¼ê°„ ì²´í¬ë¦¬ìŠ¤íŠ¸**
        - ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš” í•­ëª© ìë™ ë¶„ë¥˜
        - ê°œì„  ê¸°íšŒ í¬ì°©
        - WoW ì„±ê³¼ ë¹„êµ
        
        **3. Bayesian ë¶„ì„**
        - ì†Œì¬ë³„ ì‹¤ì œ CTR ë¶„í¬ ì¶”ì •
        - ìµœê³  ì„±ê³¼ í™•ë¥  ê³„ì‚°
        - ì‹ ë¢°ë„ í‰ê°€
        """)
    
    with col2:
        st.markdown("""
        #### ğŸ¯ í™œìš© ì‹œë‚˜ë¦¬ì˜¤
        
        **ì‹ ê·œ ìº í˜ì¸ ëŸ°ì¹­ (D+1~14)**
        - ë²¤ì¹˜ë§ˆí¬ CTR ì…ë ¥ (ì˜ˆ: ë„¤ì´ë²„ GFA 0.8%, ìœ íŠœë¸Œ 2.5%)
        - Priorë¡œ ë§¤ì²´ íŠ¹ì„± ë°˜ì˜
        - 2-3ì¼ ë°ì´í„°ë¡œ ì´ˆê¸° íŒë‹¨
        - CUSUMìœ¼ë¡œ ë¹ ë¥¸ ì´ìƒ ê°ì§€
        
        **ì •ê¸° ìš´ì˜ (D+15~)**
        - ì£¼ê°„ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¡œ ì›”ìš”ì¼ ì˜ì‚¬ê²°ì •
        - ì†Œì¬ í”¼ë¡œë„ ëª¨ë‹ˆí„°ë§
        - ì˜ˆì‚° ì¬ë¶„ë°° ê¸°íšŒ í¬ì°©
        
        **ë°ì´í„° ì¶•ì  í›„**
        - ìë™ Priorë¡œ ì „í™˜
        - ì „í™˜ ë°ì´í„° ì—°ë™í•˜ì—¬ CPI/ROAS ë¶„ì„
        """)
    
    st.markdown("---")
    st.markdown("### ğŸ’¡ ì‹œì‘ ê°€ì´ë“œ")
    
    st.markdown("""
    **1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„**
    - í•„ìˆ˜ ì»¬ëŸ¼: ë‚ ì§œ, ë§¤ì²´, ìƒí’ˆ, ì†Œì¬, ë…¸ì¶œ, í´ë¦­, ë¹„ìš©
    - í˜•ì‹: CSV, XLSX, TSV ì§€ì›
    - ìµœì†Œ ê¸°ê°„: 5ì¼ ì´ìƒ ê¶Œì¥
    
    **2ë‹¨ê³„: Prior ì„¤ì • ì„ íƒ**
    - **ìë™**: í˜„ì¬ ë°ì´í„°ë¡œ Prior ì¶”ì • (14ì¼ ì´ìƒ ë°ì´í„° ìˆì„ ë•Œ)
    - **ìˆ˜ë™**: ë§¤ì²´ë³„ ì—…ê³„ ë²¤ì¹˜ë§ˆí¬ ì…ë ¥ (2ì£¼ ë¯¸ë§Œ ë°ì´í„°ì¼ ë•Œ ê¶Œì¥)
      - ì˜ˆ: ë„¤ì´ë²„ GFA 0.8%, ìœ íŠœë¸Œ 2.5%, GDN 0.3%
    
    **3ë‹¨ê³„: ë¶„ì„ ì‹¤í–‰**
    - ì£¼ê°„ ì²´í¬ë¦¬ìŠ¤íŠ¸ì—ì„œ ì•¡ì…˜ ì•„ì´í…œ í™•ì¸
    - Bayesian ë¶„ì„ì—ì„œ í†µê³„ì  ìš°ì—´ íŒë‹¨
    - CUSUMì—ì„œ ì´ìƒ ì§•í›„ ëª¨ë‹ˆí„°ë§
    """)
    
    st.markdown("---")
    st.caption("ğŸ’¡ Tip: ì‚¬ì´ë“œë°”ì—ì„œ Prior ì„¤ì • ë°©ì‹ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")