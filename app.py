import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
from scipy.stats import beta

st.set_page_config(page_title="Ad Intelligence Pro v34.0", layout="wide")

# --- [1. ë°ì´í„° ì—”ì§„: v28.0 êµ¬ì¡° ë° ì „ì²˜ë¦¬ ìœ ì§€] ---
def load_and_clean_data(uploaded_file):
    try:
        if uploaded_file.name.endswith('.xlsx'):
            all_sheets = pd.read_excel(uploaded_file, sheet_name=None)
            df = pd.concat(all_sheets.values(), ignore_index=True)
        else:
            df = pd.read_csv(uploaded_file)
        
        df.columns = [c.strip() for c in df.columns]
        mapping = {'ë‚ ì§œ':['ë‚ ì§œ','ì¼ì'], 'ìƒí’ˆ':['ìƒí’ˆëª…','ìƒí’ˆ'], 'ì†Œì¬':['ì†Œì¬ëª…','ì†Œì¬'],
                   'ë…¸ì¶œ':['ë…¸ì¶œìˆ˜','ë…¸ì¶œ'], 'í´ë¦­':['í´ë¦­ìˆ˜','í´ë¦­'], 'ì¡°íšŒ':['ì¡°íšŒìˆ˜','ì¡°íšŒ'], 'ë¹„ìš©':['ë¹„ìš©','ì§€ì¶œ']}
        
        final_df = pd.DataFrame()
        for k, v in mapping.items():
            for col in v:
                if col in df.columns: final_df[k] = df[col]; break
        
        final_df['ë‚ ì§œ'] = pd.to_datetime(final_df['ë‚ ì§œ'], errors='coerce')
        for col in ['ë…¸ì¶œ', 'í´ë¦­', 'ì¡°íšŒ', 'ë¹„ìš©']:
            final_df[col] = pd.to_numeric(final_df[col].astype(str).str.replace(r'[^\d.]', '', regex=True), errors='coerce').fillna(0)
        
        final_df['CTR(%)'] = (final_df['í´ë¦­'] / (final_df['ë…¸ì¶œ'] + 1e-9) * 100)
        final_df['ID'] = "[" + final_df['ìƒí’ˆ'].astype(str).str.upper() + "] " + final_df['ì†Œì¬'].astype(str)
        return final_df.dropna(subset=['ë‚ ì§œ']).sort_values(['ID', 'ë‚ ì§œ'])
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì—ëŸ¬: {e}"); return pd.DataFrame()

# --- [2. í•µì‹¬ ì—”ì§„: Empirical Bayes (Moment Matching Kappa)] ---
def analyze_empirical_bayes(df):
    global_ctr = df['í´ë¦­'].sum() / (df['ë…¸ì¶œ'].sum() + 1e-9)
    # IDë³„ CTR ë¶„ì‚° ê³„ì‚° (Prior Strength ì¶”ì •ìš©)
    id_stats = df.groupby('ID').agg({'í´ë¦­':'sum', 'ë…¸ì¶œ':'sum'})
    id_ctrs = id_stats['í´ë¦­'] / (id_stats['ë…¸ì¶œ'] + 1e-9)
    var_ctr = max(id_ctrs.var(), 1e-7)
    
    # Moment Matching: kappa = [p(1-p)/var] - 1
    kappa = (global_ctr * (1 - global_ctr) / var_ctr) - 1
    kappa = np.clip(kappa, 10, 1000) # ìˆ˜ì¹˜ì  ì•ˆì •ì„± ê°€ì´ë“œ
    
    alpha_0, beta_0 = global_ctr * kappa, (1 - global_ctr) * kappa
    
    agg = id_stats.reset_index()
    agg['post_alpha'] = alpha_0 + agg['í´ë¦­']
    agg['post_beta'] = beta_0 + (agg['ë…¸ì¶œ'] - agg['í´ë¦­'])
    agg['exp_ctr'] = agg['post_alpha'] / (agg['post_alpha'] + agg['post_beta'])
    
    # Thompson Sampling
    samples = np.random.beta(agg['post_alpha'].values[:, None], 
                             agg['post_beta'].values[:, None], size=(len(agg), 5000))
    agg['prob_is_best'] = np.bincount(np.argmax(samples, axis=0), minlength=len(agg)) / 5000
    
    # í˜„ì¬ ë¹„ìš©(ìµœê·¼ 3ì¼ í‰ê· ) ê°€ì ¸ì˜¤ê¸°
    last_costs = df[df['ë‚ ì§œ'] >= df['ë‚ ì§œ'].max() - timedelta(days=3)].groupby('ID')['ë¹„ìš©'].mean()
    agg = agg.merge(last_costs, on='ID', how='left').fillna(0)
    return agg, (alpha_0, beta_0, kappa)

# --- [3. íƒì§€ ì—”ì§„: Binomial CUSUM & Bootstrap ARL] ---
def get_binomial_cusum(clicks, imps, p0, p1_ratio=0.85):
    p1 = p0 * p1_ratio
    llr = clicks * np.log(p1/p0) + (imps - clicks) * np.log((1-p1)/(1-p0))
    s = 0
    cusum = []
    for val in llr:
        s = min(0, s + val) # One-sided (í•˜ë½ ì „ìš©)
        cusum.append(s)
    return np.array(cusum)

@st.cache_data
def estimate_h_arl(p0, imps_series, target_arl=30, sims=500):
    p1 = p0 * 0.85
    llr_s, llr_f = np.log(p1/p0), np.log((1-p1)/(1-p0))
    for h in np.arange(2.0, 15.0, 1.0):
        rls = []
        for _ in range(sims):
            s, t = 0, 0
            while t < 100: # Capped ARL
                t += 1
                n = np.random.choice(imps_series) # ë…¸ì¶œìˆ˜ ë³€ë™ì„±(Bootstrap) ë°˜ì˜
                c = np.random.binomial(int(n), p0)
                s = min(0, s + (c * llr_s + (n - c) * llr_f))
                if s < -h: break
            rls.append(t)
        if np.mean(rls) >= target_arl: return h
    return 5.0

# --- [4. UI ë ˆì´ì•„ì›ƒ ë° ì •ì±… ë ˆì´ì–´] ---
uploaded_file = st.file_uploader("ìº í˜ì¸ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (CSV/XLSX)", type=['csv', 'xlsx'])

if uploaded_file:
    df = load_and_clean_data(uploaded_file)
    if not df.empty:
        ids = sorted(df['ID'].unique())
        res_agg, (a0, b0, kappa_est) = analyze_empirical_bayes(df)
        
        tabs = st.tabs(["ğŸ“Š ì„±ê³¼ ëŒ€ì‹œë³´ë“œ", "ğŸ§¬ EB Shrinkage ì§„ë‹¨", "ğŸ“‰ í•˜ë½ ê°ì§€(CUSUM)", "ğŸ¯ ì˜ˆì‚° ì •ì±… ì œì•ˆ", "ğŸ§ª ì‹œìŠ¤í…œ ë¦¬í¬íŠ¸"])

        with tabs[0]: # ëŒ€ì‹œë³´ë“œ (v28.0 UX ìœ ì§€)
            st.info("**[ê°€ì´ë“œ]** ìƒí’ˆë³„ ë¬¼ëŸ‰ ë¹„ì¤‘ê³¼ ê¸°ëŒ€ CTRì„ ë¹„êµí•©ë‹ˆë‹¤.")
            c1, c2 = st.columns(2)
            pie_m = c1.selectbox("ë¹„ì¤‘ ì§€í‘œ", ["ë¹„ìš©", "ë…¸ì¶œ", "í´ë¦­"])
            c1.plotly_chart(px.pie(df.groupby('ID')[pie_m].sum().reset_index(), values=pie_m, names='ID', hole=0.4), use_container_width=True)
            c2.plotly_chart(px.bar(res_agg, x='ID', y='exp_ctr', title="Empirical Bayes ì¶”ì • CTR(%)"), use_container_width=True)

        with tabs[1]: # EB Shrinkage ì§„ë‹¨
            st.info(f"**Prior Strength (Îº) ì¶”ì •ì¹˜: {kappa_est:.2f}**")
            st.write("ê´€ì¸¡ëœ ë°ì´í„°ì˜ ë¶„ì‚°ì„ ë°”íƒ•ìœ¼ë¡œ ê³„ì‚°ëœ ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ì…ë‹ˆë‹¤. ë°ì´í„°ê°€ ì ì„ìˆ˜ë¡ ì „ì²´ í‰ê· (Prior)ìœ¼ë¡œ ë³´ì •ë©ë‹ˆë‹¤.")
            
            fig_post = go.Figure()
            for _, row in res_agg.iterrows():
                samples = np.random.beta(row['post_alpha'], row['post_beta'], 3000)
                fig_post.add_trace(go.Box(x=samples, name=row['ID'], boxpoints=False))
            fig_post.update_layout(title="IDë³„ ì‚¬í›„ ë¶„í¬ (Posteriors)", xaxis_title="Expected CTR")
            st.plotly_chart(fig_post, use_container_width=True)

        with tabs[2]: # CUSUM í•˜ë½ ê°ì§€
            st.info("**[ê°€ì´ë“œ]** í•˜ë½ ì „ìš© ìš°ë„ë¹„ ê°ì§€ê¸° (One-sided Fatigue Detector)")
            target_id = st.selectbox("ë¶„ì„ ëŒ€ìƒ ì„ íƒ", ids)
            t_df = df[df['ID']==target_id].sort_values('ë‚ ì§œ')
            p0 = res_agg[res_agg['ID']==target_id]['exp_ctr'].values[0]
            
            # Bootstrap N-distribution ë°˜ì˜í•œ h ì‚°ì¶œ
            h_opt = estimate_h_arl(p0, t_df['ë…¸ì¶œ'].values)
            cusum_v = binomial_cusum(t_df['í´ë¦­'], t_df['ë…¸ì¶œ'], p0)
            is_alarm = cusum_v[-1] < -h_opt
            
            fig_c = go.Figure()
            fig_c.add_trace(go.Scatter(x=t_df['ë‚ ì§œ'], y=cusum_v, name="Log-Likelihood Ratio Sum", fill='tozeroy'))
            fig_c.add_hline(y=-h_opt, line_dash="dash", line_color="red", annotation_text=f"Capped ARL-30 Target (h={h_opt})")
            st.plotly_chart(fig_c, use_container_width=True)
            if is_alarm: st.error("ğŸš¨ **êµ¬ì¡°ì  í•˜ë½ ê°ì§€**: ì„±ê³¼ê°€ í†µê³„ì  ì‹ ë¢° í•œê³„ë¥¼ ë²—ì–´ë‚˜ í•˜ë½ ì¤‘ì…ë‹ˆë‹¤.")

        with tabs[3]: # ì˜ˆì‚° ìµœì í™” (Expected Score ê¸°ë°˜)
            st.info("**[ê°€ì´ë“œ]** ê¸°ëŒ€ CTR ë° ìŠ¹ë¦¬ í™•ë¥  ê°€ì¤‘ì¹˜ë¥¼ ê²°í•©í•œ ìì› ë°°ë¶„")
            if st.button("ì˜ˆì‚° ì •ì±… ì‹¤í–‰"):
                # Policy: (ê¸°ëŒ€ ì„±ê³¼ * ìš°ìˆ˜ í™•ë¥ ) / í‰ê·  ì ìˆ˜ ê¸°ë°˜ ì¡°ì •
                res_agg['score'] = res_agg['exp_ctr'] * res_agg['prob_is_best']
                avg_score = res_agg['score'].mean() + 1e-9
                res_agg['proposed'] = res_agg['ë¹„ìš©'] * (res_agg['score'] / avg_score)
                
                # Safety Rail (Budget Inertia): ì „ì¼ ëŒ€ë¹„ Â±30% ì œí•œ
                res_agg['final_proposed'] = res_agg.apply(lambda r: np.clip(r['proposed'], r['ë¹„ìš©']*0.7, r['ë¹„ìš©']*1.3), axis=1)
                
                st.table(res_agg[['ID', 'exp_ctr', 'prob_is_best', 'final_proposed']].style.format(
                    {'exp_ctr':'{:.4f}', 'prob_is_best':'{:.2f}', 'final_proposed':'{:,.0f}'}))

        with tabs[4]: # ì‹œìŠ¤í…œ ë¦¬í¬íŠ¸ (ìš©ì–´ ë° ì—„ë°€ì„± ëª…ì‹œ)
            st.subheader("ğŸ“Š Methodological Transparency")
            st.write("""
            - **Estimation**: Empirical Bayes (Moment Matching). Îº is derived from global variance.
            - **Detection**: Binomial Log-Likelihood Ratio CUSUM. One-sided for decay detection.
            - **Thresholding**: Monte Carlo-estimated Capped ARL (Max 100d). 
            - **Exposure Variance**: Bootstrap sampling from historical 'N' distribution during ARL simulation.
            """)
            st.success("ì´ ì‹œìŠ¤í…œì€ ë‹¨ìˆœ ê°€ì†ë„(Heuristic)ë¥¼ ë°°ì œí•˜ê³  í™•ë¥ ë¡ ì  ìš°ë„ ê²€ì •(Likelihood Test)ì„ ë”°ë¦…ë‹ˆë‹¤.")